{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3016b08a-7cff-42cb-a17d-63d5bb92e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e921f0-d79b-43f3-8feb-56f3651e8d54",
   "metadata": {},
   "source": [
    "# Basic Residual Block \n",
    "- ResNet 18, ResNet 34\n",
    "- 논문에서 볼 수 있는 것과 같이 input x를 identity로 사용\n",
    "- projection하는 과정에서 문제가 존재\n",
    "    - stride가 1이 아니어서 feature map의 size가 다른 경우\n",
    "    - input channel 수와 conv layer를 거친 output channel수가 다른 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a2e6d0-8595-4a0f-929d-622b079ddc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride = 1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        #stride를 통해 너비와 높이 조절\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size = 3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels*BasicBlock.expansion)\n",
    "        )\n",
    "        \n",
    "        # identity mapping \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # projection mapping using 1*1 conv\n",
    "        # input과 conv layer를 거친 feature map size와 channel수가 다른 경우 맞추어주기 위해 추가\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.residual_function(x) + self.shortcut(x)\n",
    "            x = self.relu(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da94904-7adf-4672-b940-07facf0971d1",
   "metadata": {},
   "source": [
    "# BottleNeck\n",
    "- resnet 50,101, 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5bc30d1-a563-4582-b10e-60d5d59ae7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels* BottleNeck.expansion),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf1814-4a73-4ad7-9c0d-e35ee8d89181",
   "metadata": {},
   "source": [
    "# ResNet 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d09243a-8e77-447a-95e1-9cddacf6a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes = 10, init_weights = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # conv2_x는 max_pool layer를 앞에서 거치기 때문에 stride가 2인 경우 x\n",
    "        # conv3_x부터는 첫 번째 conv layer에서 feature map size를 줄여주기 때문에 2로 설정\n",
    "        self.conv2_x = self.make_layer(block, 64, num_block[0], 1) # output channel 64\n",
    "        self.conv3_x = self.make_layer(block, 128, num_block[1], 2) # output channel 128\n",
    "        self.conv4_x = self.make_layer(block, 256, num_block[2], 2) # output channel 256\n",
    "        self.conv5_x = self.make_layer(block, 512, num_block[3], 2) # output channel 512\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "        \n",
    "    # 각 block의 첫번째 conv layer에서만 stride=2로 지정하여 feature map의 크기를 반으로 줄여준다.\n",
    "    # 주의 : 정수의 합이 아닌 리스트의 합\n",
    "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1) \n",
    "        # ex) [1, 1], [2, 1, 1], [2, 1, 1, 1]....\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            # 하나의 layer가 끝나게 되면 channel수를 expansion의 값에 따라서 증가시켜준다.\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        x = self.conv3_x(output)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "     \n",
    "    # weight 초기화 함수 정의 \n",
    "    def _initialize_weights(self):\n",
    "        #모델의 모듈을 차례대로 불러온다.\n",
    "        for m in self.modules():\n",
    "            #isinstance() -> 차례로 layer을 입력하여, layer의 형태를 반환(nn.Conv2d, nn.BatchNorm2d ...)\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                # Kaming Initialization\n",
    "                # 모듈의 가중치를 kaming he normal로 초기화합니다.\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                #torch.nn.init.constant_(tensor, val) -> tensor을 val로 초기화\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                #torch.nn.init.normal_(tensor, mean=0.0, std=1.0) -> tensor을 mean, std의 normal distrubution으로 초기화\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0) \n",
    "        \n",
    "# block의 종류, num_block 입력\n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock,[2,2,2,2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet(BasicBlock,[3,4,6,3])\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BottleNeck,[3,4,6,3])\n",
    "\n",
    "def resnet101():\n",
    "    return ResNet(BottleNeck,[3,4,23,3])\n",
    "\n",
    "def resnet152():\n",
    "    return ResNet(BottleNeck,[3,8,36,3])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ab528a-6305-49ce-9570-5a924611ead7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "#네트워크 생성\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = resnet50().to(device)\n",
    "x = torch.randn(3, 3, 224, 224).to(device)\n",
    "output = model(x)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09dd9ca9-37da-4281-9bc2-4d2b8c338b94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "           Conv2d-11          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 64, 64]             512\n",
      "             ReLU-13          [-1, 256, 64, 64]               0\n",
      "           Conv2d-14          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-15          [-1, 256, 64, 64]             512\n",
      "             ReLU-16          [-1, 256, 64, 64]               0\n",
      "       BottleNeck-17          [-1, 256, 64, 64]               0\n",
      "           Conv2d-18           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-19           [-1, 64, 64, 64]             128\n",
      "             ReLU-20           [-1, 64, 64, 64]               0\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "             ReLU-23           [-1, 64, 64, 64]               0\n",
      "           Conv2d-24          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-25          [-1, 256, 64, 64]             512\n",
      "             ReLU-26          [-1, 256, 64, 64]               0\n",
      "             ReLU-27          [-1, 256, 64, 64]               0\n",
      "       BottleNeck-28          [-1, 256, 64, 64]               0\n",
      "           Conv2d-29           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-30           [-1, 64, 64, 64]             128\n",
      "             ReLU-31           [-1, 64, 64, 64]               0\n",
      "           Conv2d-32           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-33           [-1, 64, 64, 64]             128\n",
      "             ReLU-34           [-1, 64, 64, 64]               0\n",
      "           Conv2d-35          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-36          [-1, 256, 64, 64]             512\n",
      "             ReLU-37          [-1, 256, 64, 64]               0\n",
      "             ReLU-38          [-1, 256, 64, 64]               0\n",
      "       BottleNeck-39          [-1, 256, 64, 64]               0\n",
      "           Conv2d-40          [-1, 128, 64, 64]          32,768\n",
      "      BatchNorm2d-41          [-1, 128, 64, 64]             256\n",
      "             ReLU-42          [-1, 128, 64, 64]               0\n",
      "           Conv2d-43          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-44          [-1, 128, 32, 32]             256\n",
      "             ReLU-45          [-1, 128, 32, 32]               0\n",
      "           Conv2d-46          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-48          [-1, 512, 32, 32]               0\n",
      "           Conv2d-49          [-1, 512, 32, 32]         131,072\n",
      "      BatchNorm2d-50          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-51          [-1, 512, 32, 32]               0\n",
      "       BottleNeck-52          [-1, 512, 32, 32]               0\n",
      "           Conv2d-53          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-54          [-1, 128, 32, 32]             256\n",
      "             ReLU-55          [-1, 128, 32, 32]               0\n",
      "           Conv2d-56          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 32, 32]             256\n",
      "             ReLU-58          [-1, 128, 32, 32]               0\n",
      "           Conv2d-59          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-60          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-61          [-1, 512, 32, 32]               0\n",
      "             ReLU-62          [-1, 512, 32, 32]               0\n",
      "       BottleNeck-63          [-1, 512, 32, 32]               0\n",
      "           Conv2d-64          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-65          [-1, 128, 32, 32]             256\n",
      "             ReLU-66          [-1, 128, 32, 32]               0\n",
      "           Conv2d-67          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-68          [-1, 128, 32, 32]             256\n",
      "             ReLU-69          [-1, 128, 32, 32]               0\n",
      "           Conv2d-70          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-71          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-72          [-1, 512, 32, 32]               0\n",
      "             ReLU-73          [-1, 512, 32, 32]               0\n",
      "       BottleNeck-74          [-1, 512, 32, 32]               0\n",
      "           Conv2d-75          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-76          [-1, 128, 32, 32]             256\n",
      "             ReLU-77          [-1, 128, 32, 32]               0\n",
      "           Conv2d-78          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-79          [-1, 128, 32, 32]             256\n",
      "             ReLU-80          [-1, 128, 32, 32]               0\n",
      "           Conv2d-81          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-82          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-83          [-1, 512, 32, 32]               0\n",
      "             ReLU-84          [-1, 512, 32, 32]               0\n",
      "       BottleNeck-85          [-1, 512, 32, 32]               0\n",
      "           Conv2d-86          [-1, 256, 32, 32]         131,072\n",
      "      BatchNorm2d-87          [-1, 256, 32, 32]             512\n",
      "             ReLU-88          [-1, 256, 32, 32]               0\n",
      "           Conv2d-89          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 16, 16]             512\n",
      "             ReLU-91          [-1, 256, 16, 16]               0\n",
      "           Conv2d-92         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-93         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-94         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-95         [-1, 1024, 16, 16]         524,288\n",
      "      BatchNorm2d-96         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-97         [-1, 1024, 16, 16]               0\n",
      "       BottleNeck-98         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-99          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-100          [-1, 256, 16, 16]             512\n",
      "            ReLU-101          [-1, 256, 16, 16]               0\n",
      "          Conv2d-102          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-103          [-1, 256, 16, 16]             512\n",
      "            ReLU-104          [-1, 256, 16, 16]               0\n",
      "          Conv2d-105         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-106         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-107         [-1, 1024, 16, 16]               0\n",
      "            ReLU-108         [-1, 1024, 16, 16]               0\n",
      "      BottleNeck-109         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-110          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-111          [-1, 256, 16, 16]             512\n",
      "            ReLU-112          [-1, 256, 16, 16]               0\n",
      "          Conv2d-113          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-114          [-1, 256, 16, 16]             512\n",
      "            ReLU-115          [-1, 256, 16, 16]               0\n",
      "          Conv2d-116         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-117         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-118         [-1, 1024, 16, 16]               0\n",
      "            ReLU-119         [-1, 1024, 16, 16]               0\n",
      "      BottleNeck-120         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-121          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 16, 16]             512\n",
      "            ReLU-123          [-1, 256, 16, 16]               0\n",
      "          Conv2d-124          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 16, 16]             512\n",
      "            ReLU-126          [-1, 256, 16, 16]               0\n",
      "          Conv2d-127         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-129         [-1, 1024, 16, 16]               0\n",
      "            ReLU-130         [-1, 1024, 16, 16]               0\n",
      "      BottleNeck-131         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-132          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-133          [-1, 256, 16, 16]             512\n",
      "            ReLU-134          [-1, 256, 16, 16]               0\n",
      "          Conv2d-135          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-136          [-1, 256, 16, 16]             512\n",
      "            ReLU-137          [-1, 256, 16, 16]               0\n",
      "          Conv2d-138         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-139         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-140         [-1, 1024, 16, 16]               0\n",
      "            ReLU-141         [-1, 1024, 16, 16]               0\n",
      "      BottleNeck-142         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-143          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-144          [-1, 256, 16, 16]             512\n",
      "            ReLU-145          [-1, 256, 16, 16]               0\n",
      "          Conv2d-146          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-147          [-1, 256, 16, 16]             512\n",
      "            ReLU-148          [-1, 256, 16, 16]               0\n",
      "          Conv2d-149         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-150         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-151         [-1, 1024, 16, 16]               0\n",
      "            ReLU-152         [-1, 1024, 16, 16]               0\n",
      "      BottleNeck-153         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-154          [-1, 512, 16, 16]         524,288\n",
      "     BatchNorm2d-155          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-156          [-1, 512, 16, 16]               0\n",
      "          Conv2d-157            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-158            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-159            [-1, 512, 8, 8]               0\n",
      "          Conv2d-160           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-161           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-162           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-163           [-1, 2048, 8, 8]       2,097,152\n",
      "     BatchNorm2d-164           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-165           [-1, 2048, 8, 8]               0\n",
      "      BottleNeck-166           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-167            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-168            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-169            [-1, 512, 8, 8]               0\n",
      "          Conv2d-170            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-171            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-172            [-1, 512, 8, 8]               0\n",
      "          Conv2d-173           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-174           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-175           [-1, 2048, 8, 8]               0\n",
      "            ReLU-176           [-1, 2048, 8, 8]               0\n",
      "      BottleNeck-177           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-178            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-179            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-180            [-1, 512, 8, 8]               0\n",
      "          Conv2d-181            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-182            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-183            [-1, 512, 8, 8]               0\n",
      "          Conv2d-184           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-185           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-186           [-1, 2048, 8, 8]               0\n",
      "            ReLU-187           [-1, 2048, 8, 8]               0\n",
      "      BottleNeck-188           [-1, 2048, 8, 8]               0\n",
      "AdaptiveAvgPool2d-189           [-1, 2048, 1, 1]               0\n",
      "          Linear-190                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 429.27\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 519.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 256, 256), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc35fd-4e8c-447b-b804-c0bae17c97fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
